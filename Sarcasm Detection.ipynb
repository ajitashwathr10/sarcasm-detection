{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c09d049-64dd-4d0d-8fc0-ea017daa4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e4c7c64-1434-4b74-a722-e15a57d24ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AJIT ASHWATH\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJIT ASHWATH\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\AJIT ASHWATH\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AJIT ASHWATH R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f54a28f-7be0-49bb-82f8-327ef00a39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b547c43-d936-47bf-aa2f-bcfc7435856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68572999-3f27-49f6-b054-12c42d060b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSarcasmClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super(BERTSarcasmClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        return self.linear(dropout_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71053f22-9ea6-4937-9cf9-14d52baeec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDetector:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = None\n",
    "        self.traditional_model = None\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def prepare_data(self, file_path):\n",
    "        try:\n",
    "            # First attempt: Read as JSON Lines\n",
    "            data_list = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data_list.append(json.loads(line))\n",
    "            data = pd.DataFrame(data_list)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # Second attempt: Read as single JSON array\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = pd.read_json(f)\n",
    "            except Exception as e2:\n",
    "                print(f\"Error reading JSON file: {e2}\")\n",
    "                return None\n",
    "        \n",
    "        # Preprocess headlines\n",
    "        data['processed_headline'] = data['headline'].apply(self.preprocessor.preprocess)\n",
    "        \n",
    "        print(f\"Loaded {len(data)} records\")\n",
    "        print(\"Data sample:\")\n",
    "        print(data.head())\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def train_traditional_model(self, X_train, y_train):\n",
    "        self.traditional_model = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "        ])\n",
    "        self.traditional_model.fit(X_train, y_train)\n",
    "\n",
    "    def evaluate_bert(self, val_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return correct / total\n",
    "\n",
    "    def train_bert_model(self, train_loader, val_loader, epochs=3):\n",
    "        self.model = BERTSarcasmClassifier().to(self.device)\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"Batch [{batch_idx + 1}/{len(train_loader)}] \"\n",
    "                          f\"Loss: {loss.item():.4f} \"\n",
    "                          f\"Accuracy: {100 * correct/total:.2f}%\")\n",
    "            \n",
    "            val_accuracy = self.evaluate_bert(val_loader)\n",
    "            print(f'Epoch {epoch + 1} Summary:')\n",
    "            print(f'Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "            print(f'Training Accuracy: {100 * correct/total:.2f}%')\n",
    "            print(f'Validation Accuracy: {100 * val_accuracy:.2f}%')\n",
    "\n",
    "    def predict(self, text, model_type='ensemble'):\n",
    "        processed_text = self.preprocessor.preprocess(text)\n",
    "        \n",
    "        if model_type == 'traditional' or (model_type == 'ensemble' and self.model is None):\n",
    "            return self.traditional_model.predict([processed_text])[0]\n",
    "        \n",
    "        elif model_type == 'bert' or model_type == 'ensemble':\n",
    "            self.model.eval()\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                processed_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "            attention_mask = encoding['attention_mask'].to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "            if model_type == 'bert':\n",
    "                return predicted.item()\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            traditional_pred = self.traditional_model.predict([processed_text])[0]\n",
    "            bert_pred = predicted.item()\n",
    "            return int((traditional_pred + bert_pred) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e123a5-f381-43ad-8b65-c1be290f1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading and preparing data...\n",
      "Loaded 26709 records\n",
      "Data sample:\n",
      "                                        article_link  \\\n",
      "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
      "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
      "2  https://local.theonion.com/mom-starting-to-fea...   \n",
      "3  https://politics.theonion.com/boehner-just-wan...   \n",
      "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
      "\n",
      "                                            headline  is_sarcastic  \\\n",
      "0  former versace store clerk sues over secret 'b...             0   \n",
      "1  the 'roseanne' revival catches up to our thorn...             0   \n",
      "2  mom starting to fear son's web series closest ...             1   \n",
      "3  boehner just wants wife to listen, not come up...             1   \n",
      "4  j.k. rowling wishes snape happy birthday in th...             0   \n",
      "\n",
      "                                  processed_headline  \n",
      "0  former versace store clerk sue secret black co...  \n",
      "1  roseanne revival catch thorny political mood b...  \n",
      "2  mom starting fear son web series closest thing...  \n",
      "3  boehner want wife listen come alternative debt...  \n",
      "4   jk rowling wish snape happy birthday magical way  \n",
      "\n",
      "Training traditional model...\n",
      "\n",
      "Preparing BERT datasets...\n",
      "\n",
      "Training BERT model...\n",
      "\n",
      "Epoch 1/3\n",
      "Batch [10/668] Loss: 0.5582 Accuracy: 54.69%\n",
      "Batch [20/668] Loss: 0.6699 Accuracy: 51.09%\n",
      "Batch [30/668] Loss: 0.6733 Accuracy: 50.73%\n",
      "Batch [40/668] Loss: 0.6488 Accuracy: 52.73%\n",
      "Batch [50/668] Loss: 0.6536 Accuracy: 54.69%\n",
      "Batch [60/668] Loss: 0.5749 Accuracy: 57.03%\n",
      "Batch [70/668] Loss: 0.6217 Accuracy: 58.66%\n",
      "Batch [80/668] Loss: 0.4665 Accuracy: 60.27%\n",
      "Batch [90/668] Loss: 0.5343 Accuracy: 61.67%\n",
      "Batch [100/668] Loss: 0.4510 Accuracy: 63.03%\n",
      "Batch [110/668] Loss: 0.5102 Accuracy: 64.09%\n",
      "Batch [120/668] Loss: 0.5040 Accuracy: 64.77%\n",
      "Batch [130/668] Loss: 0.5252 Accuracy: 65.48%\n",
      "Batch [140/668] Loss: 0.4854 Accuracy: 66.09%\n",
      "Batch [150/668] Loss: 0.5156 Accuracy: 66.75%\n",
      "Batch [160/668] Loss: 0.4903 Accuracy: 67.09%\n",
      "Batch [170/668] Loss: 0.3199 Accuracy: 67.74%\n",
      "Batch [180/668] Loss: 0.3645 Accuracy: 68.35%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        detector = SarcasmDetector()\n",
    "        print(\"Loading and preparing data...\")\n",
    "        data = detector.prepare_data(\"C:\\\\Users\\\\AJIT ASHWATH R\\\\Downloads\\\\Sarcasm.json\")\n",
    "        \n",
    "        if data is None:\n",
    "            print(\"Failed to load data. Please check the file path and format.\")\n",
    "            return\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data['processed_headline'],\n",
    "            data['is_sarcastic'],\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTraining traditional model...\")\n",
    "        detector.train_traditional_model(X_train, y_train)\n",
    "        \n",
    "        print(\"\\nPreparing BERT datasets...\")\n",
    "        train_dataset = SarcasmDataset(X_train.values, y_train.values, detector.tokenizer)\n",
    "        val_dataset = SarcasmDataset(X_test.values, y_test.values, detector.tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "        \n",
    "        print(\"\\nTraining BERT model...\")\n",
    "        detector.train_bert_model(train_loader, val_loader)\n",
    "        \n",
    "        # Test predictions\n",
    "        test_texts = [\n",
    "            \"Scientists cure cancer with one simple trick\",\n",
    "            \"New study shows benefits of exercise\",\n",
    "            \"Area man becomes expert in everything after reading single article\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting predictions:\")\n",
    "        for text in test_texts:\n",
    "            prediction = detector.predict(text, model_type='ensemble')\n",
    "            print(f\"\\nText: {text}\")\n",
    "            print(f\"Prediction: {'Sarcastic' if prediction == 1 else 'Not Sarcastic'}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure all required libraries are installed and the data file path is correct.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543aaec1-2e35-46a3-9f98-f896a837f23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ddb7f-90c2-4fc0-a221-66460d3393a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
